\section{Introduction}
\label{sec:intro}

% General Introduction
In search for learning methods that can gracefully scale to large domains, the
Reinforcement Learning (RL) community has been looking at various hierarchical
learning schemes \cite{BartoMahadevan}. The options framework proposed by
Sutton, Precup and Singh \cite{Sutton1998} provides extended actions for which a
policy is already learnt, reducing the complexity of the learning task, and
generally making the learning task faster. An open question in the options
framework is discovering the options themselves. There has been substantial work
to learn options, mainly focussed around identifying ``bottleneck'' states,
either empirically as in \cite{Stolle}, or more recently, using graph
theoretic methods like betweeness \cite{Simsek} or graph partitions
\cite{Simsek2005}.

% Motivation
In this work, we propose a method for creating options motivated from a
cognitive perspective, based on the following hypothesis: we memorise many
actions, not necessarily bottleneck ones, and evolve them. Based on their
necessity in solving problems these actions are either reinforced, or gradually
forgotten. The actions could be of varying complexity, and it is intuitive to
expect that we probably learn a great deal more {\em simple} actions than
complex ones. In context of the options framework, these actions correspond to
options, and ``complex actions'' correspond to longer options.

% Our options
A desirable set of options gives the agent a set of skills which can be put
together to efficiently accomplish almost any task. From the perspective of the
state-space interaction graph, this is similar to the problem of distributed
search studied by Kleinberg \cite{Kleinberg}; adding edges to a graph such that
any node can be efficiently reached. Guided by this intuition, the method we
propose generates options using a generalisation of the inverse-square law,
along the lines of the small-world graph generation model proposed by Kleinberg.

% Summary of the results
Our results show that agents trained using our `small-world' options indeed
perform well, and converge to optimal performance quickly and with little
variance. 

