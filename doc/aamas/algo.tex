\section{Constructing Options from Experience} 
\label{sec:algo}

In \secref{sec:theory}, we remarked that $O(|S|)$ options may need to be
generated. Given the scale of this number, we require an algorithm to
efficiently generate options within a budget of training epochs in order
for small world options to be practical. On the other hand, drawing
insight from the proof of \thmref{thm:small-world}, the objective of the
options is to bring the agent into an exponentially smaller
neighbourhood of the maximal value state, thus we should be able to get
away with far cheaper options.

The algorithm (\algoref{algo:small-world-experience}) we propose takes
a given MDP $\mdp$, and trains an agent to learn $T$ different tasks
(i.e. different $\rewards$) on it, evenly dividing the epoch budget
amongst them. With each learned task, we certainly have a good policy
for path options from any state to the state of maximal value, $M_v$.
However, we observe that will also have a good policy for path options
from $u$ to $v$ is the path is `along the gradient' of $Q$, i.e. when
$V(u) < V(v) < V(M_v)$. Observing that $V(s) \approx \argmax_{v}
Q(s,\pi(s))$, we detail the algorithm to construct options from the
$Q$-value function in \algoref{algo:qoptions}. We use this algorithm to
construct many options from a single task solution.

\begin{algorithm}[H]
  \caption{Small World Options from Experience}
  \label{algo:small-world-experience}
  \begin{algorithmic}[1]
      \REQUIRE $\mdp$, $\Rewards$, $k$, $n$, epochs, tasks
      \STATE $O \gets \emptyset$
      \FOR{ $i= 0 \to \textrm{tasks}$ }
        \STATE $\rewards \sim \Rewards$
        \STATE $Q \gets $ Solve $\mdp$ with $\rewards$ using
            $\frac{\textrm{epochs}}{\textrm{tasks}}$ epochs
        \STATE $O' \gets $ QOptions( $Q$, $k$,
            $\frac{n}{\textrm{tasks}}$ )
        \STATE $O \gets O \cup O'$
      \ENDFOR
      \RETURN A random subset of $n$ options from $O$
  \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
  \caption{{\bf QOptions}: Options from a $Q$-Value Function}
  \label{algo:qoptions}
  \begin{algorithmic}[1]
      \REQUIRE $Q$, $k$, $n$
      \STATE $O \gets \emptyset$
      \STATE $\pi \gets $ greedy policy from $Q$
      \FORALL{ $s$ in $\states$ }
        \STATE Choose an $s'$ according to $P_k$
        \IF{ $Q(s', \pi(s')) > Q(s, \pi(s))$ }
          \STATE $O \gets O \cup \tuple{\{s\}, \pi, \{s'\} \cup \{t \mid Q(s',\pi(s')) < Q(t, \pi(t))\} }$
        \ENDIF
      \ENDFOR{ $s$ in $\states$ }
      \RETURN A random subset of $n$ options from $O$
  \end{algorithmic}
\end{algorithm}

We note here except for sampling $s'$ from $P_k$, we do not require any knowledge of the MDP, nor do we need to construct a local model of the same. $s'$ can approximately be sampled using $\frac{E[l]}{\log(n)}$ in place of $P_k$. 

