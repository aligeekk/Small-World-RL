\section{Small World Options}
\label{sec:theory}

% Constructing Options 
We construct an option `short-circuiting' two states using a policy constructed
from the shortest path on this graph. For every state $x$, we select a state to
be short-circuited $y$ with using a multinomial distribution with weight
proportional to the distance between them in the state space, i.e. $w(x,y)
\propto d(x,y)^{-r}$. 

Another example of constructing an option on this graph would be to
define a policy that takes any state to a particular one along the
shortest path.  This is the approach adopted by Simsek and Barto in
\cite{Simsek2008}, where local maxima of the betweenness scores are used
to identify bottlenecks, and options defined to reach these bottlenecks
optimally from any state.
% Introduction and motivation for the proof
For a given MDP $\mdp$, let $\graph$ be the state-interaction graph, and
consider `path options' to be additional edges in this graph. The
decisions made by an $\epsilon$-greedy agent in $\mdp$ is very similar
to those made by a distributed $\epsilon$-greedy algorithm searching for
the maxima of the value function $\Vf$ embedded in $\graph$. 

Using this intuition, we wish to show that given small world options,
an $\epsilon$-greedy agent will be able reach to the maxima of the
optimal value function in $O( (\log n)^2 )$ decision epochs. This
statement is a corollary of the following theorem,

\begin{theorem}
    \label{thm:small-world}
    Let $f : V \to \Re$ be a function embedded on the graph
    $\graph(V,E)$, such that, $\kappa_1 \|u-v\| - c_1 \le \|f(u) -
    f(v)\| \le \kappa_2 \|u - v\| - c_2$, where $0 \le \kappa_1 \le
    \kappa_2$, and $0 \le c_2 \le \frac{c_1}{2}$. Let \egreedyalgo be
    an algorithm which chooses with probability $1-\epsilon$ to
    transit to the neighbouring state closest to a global maxima of
    $f$, $M_f$, i.e. $N(u) = \argmin_v \|f(v) - f(M_f)\|$ (i.e.
    $\epsilon$-greedy algorithm with respect to $f$).
    
    If $\graph(V,E)$ is $k$-dimensional lattice, and contains edges
    distributed according to the inverse power law $p(u,v) \propto
    \|u-v\|^{-k}$, then \egreedyalgo takes $O( (\log n)^2 )$ decisions
    to reach $M_f$.
\end{theorem}
\begin{proof}
    Refer \appendixref{sec:small-worlds}.
    \\ \qed
\end{proof}

We are left to prove that $\Vf$ does satisfy the above property.
First, let us formalise the notion of a `path' option.

\begin{definition}
    A {\em robust path option} $o(u,v)$, where $u,v \in \states$ is an
    option that takes the agent from $u$ to $v$ `robustly', in the
    sense that in each epoch, the agent moves closer to $v$ with a
    probability $1-\epsilon > \frac{1}{2}$. \footnote{This condition
    is equivalent to saying that the option takes the agent from $u$
    to $v$ in finite time, and hence is not particularly strong.}.
    Note that this $\epsilon$ includes any environmental effects as
    well.
\end{definition}

The following lemma shows that $\Vf$ satisfies the properties of a
embedded function required for \thmref{thm:small-world}.

\begin{lemma}
    \label{lm:distance}
    Let $o(u,v)$ be the preferred option in state $u$, and let $\|u -
    v\|_V = |\log \Vf(v) - \log \Vf(u)|$. Then, 
    \begin{IEEEeqnarray*}{rCCCl}
        k_1 \|u - v\| - c_1 & \le & \|u - v\|_V & \le & k_2 \|u - v\|, 
    \end{IEEEeqnarray*}
    \noindent
    where $k_1 = \log \frac{1}{\gamma} $, $k_2 = \log
    \frac{!}{(1-\epsilon)\gamma}$, and $c_1 = \log
    \frac{1}{1-\gamma}$.
\end{lemma}
\begin{proof}

    For notational convenience, let $\epsilonm = 1 - \epsilon$. From the Bellman optimality condition, we get the value of $o(u,v)$ to be,
    \begin{eqnarray*}
        \Qf(u, o(u,v)) &=& \E_{l}[ \gamma^{l} \Vf(v) + \sum_{i=1}^{l} \gamma^{i-1} r_i ],
    \end{eqnarray*}
    \noindent
    where $l$ is the length of the option, and $r_i$ is the reward
    obtained in the $i$-th step of following the option. 
    
    If o(u,v) is the preferred option in state $u$, then $\Vf(u) =
    \Qf(u, o(u,v))$.  Using the property that $0 \le r_i \le 1$,
    \begin{IEEEeqnarray*}{rCCCl}
        \E_{l}[ \gamma^{l} \Vf(v) ] &\le& \Vf(u) &\le& \E_{l}[ \gamma^{l} \Vf(v) + \sum_{i=1}^{l} \gamma^{i-1}] \\
        \E_{l}[ \gamma^{l} ] \Vf(v) &\le& \Vf(u) &\le& \E_{l}[ \gamma^{l} ] \Vf(v) + \frac{1}{1 - \gamma}. \IEEEyesnumber \label{eq:v-bound}
    \end{IEEEeqnarray*}

    $\E_{l}$ is an expectation over the length of the option. Using
    the property that $o(u,v)$ is robust, we move closer to $v$ with
    probability $\epsilonm$; this is exactly the setting of the
    well-studied gambler's ruin problem, where the gambler begins with
    a budget of $\|u-v\|$, and wins with a probability of $\epsilon$.

    \begin{lemma}
        Consider the gambler's ruin problem where the gambler begins 
        with a budget of $m$, and has a winning probability of $q < \frac{1}{2}$
        (the dealer has a winning probability of $p=1-q$). Let $L$ be
        a random variable for the length of the game. Then, 
        \begin{eqnarray*}
            g_m(x) = \sum_{l=0}^{\infty} P(L = l) x^{l} &=& \frac{1}{\lambda_1^m( x ) + \lambda_2^m( x )},
        \end{eqnarray*}
        \noindent
        where $\lambda_1(x) = \frac{1 + \sqrt{1 - 4pq x^2}}{2px}$, and
        $\lambda_2(x) = \frac{1 - \sqrt{1 - 4pq x^2}}{2px}$.

        Further, when $x \le 1$,
        \begin{IEEEeqnarray*}{rCCCl}
            (px)^m  &\le&  g_m(x) &\le& x^m.
        \end{IEEEeqnarray*}
    \end{lemma}
    \begin{proof}

        For the first part. refer \cite{Feller1968}.

        The second part follows as a corollary, 
        \begin{IEEEeqnarray*}{rCCCl}
            \frac{1}{ (\lambda_1( x ) + \lambda_2( x ) )^{m} }  &\le&  g_m(x) &\le& \sum_{l=m}^{\infty} P(L = l) x^{l} \\
            \frac{1}{(\frac{2}{2px})^m}  &\le&  g_m(x) &\le& \sum_{l=m}^{\infty} P(L = l) x^{m} \\
            (px)^m  &\le&  g_m(x) &\le& x^m.
        \end{IEEEeqnarray*}
        \qed
    \end{proof}

    In our situation, $p = 1 - \epsilon = \epsilonm > \frac{1}{2}$, $q = \epsilon$, and $m = \|u - v\|$.     
    \begin{IEEEeqnarray*}{rCCCl}
        (\epsilonm \gamma)^{\|u-v\|} &\le& \E_{l}[ \gamma^{l} ] &\le& (\gamma)^{\|u-v\|}.
    \end{IEEEeqnarray*}

    Returning to \eqnref{eq:v-bound},
    \begin{IEEEeqnarray*}{rCCCl}
        \E_{l}[ \gamma^{l} ] \Vf(v) &\le& \Vf(u) &\le& \E_{l}[ \gamma^{l} ] \Vf(v) + \frac{1}{1 - \gamma} \\
        (\epsilonm \gamma)^{\|u-v\|} \Vf(v) &\le& \Vf(u) &\le& \gamma^{\|u-v\|} \Vf(v) + \frac{1}{1 - \gamma} \\
        \|u-v\| \log (\epsilonm \gamma) + \log \Vf(v) &\le& \log \Vf(u) &\le& \|u-v\| \log \gamma + \log \Vf(v) + \log \frac{1}{1 - \gamma} \\
        \|u-v\| \log \frac{1}{\gamma} - \log \frac{1}{1-\gamma} &\le& \|u - v\|_V &\le& \|u-v\| \log (\frac{1}{\epsilonm \gamma}).
    \end{IEEEeqnarray*}
    \qed
\end{proof}

Thus, an $\epsilon$-greedy agent with small-world options will reach
the maximal value function in $O(\log n)$ decisions.

