"""
RL Framework
Authors: Arun Chaganty
Environment Base Class; Represented by an MDP
"""

import random
import numpy as np
import networkx as nx
import util
import pdb

class Environment:
    """Environment represented as an MDP"""
    S = 0
    A = 0
    P = []
    R = {}
    R_bias = 0
    Q = []

    state = 0

    def __init__( self, S, A, P, R, R_bias, start_set, end_set ):
        self.S = S 
        self.A = A 
        self.P = P
        self.R = R
        self.R_bias = R_bias
        self.start_set = start_set
        self.end_set = end_set

        # State action set for MDP
        Q = []
        for s in xrange(self.S):
            Q.append( tuple( ( a for a in xrange( self.A ) if len( self.P[ a ][ s ] ) > 0 ) ) )
        self.Q = Q

    def start( self ):
        """Calls _start - this is to support Options later"""
        return self._start()

    def _start(self):
        """Initialise the Environment
        @returns initial state and valid actions
        """
        if self.start_set:
            state = random.choice( self.start_set )
        else:
            state = np.random.randint( self.S )
            while len( self.Q[ state ] ) == 0:
                state = np.random.randint( self.S )
        self.state = state

        return state

    def react(self, action):
        return self._react( action )

    def _react(self, action):
        state = util.choose( self.P[ action ][ self.state ] )
        reward = self.R.get( (self.state, state), 0 ) + self.R_bias

        # If there is no way to get out of this state, the episode has ended
        if self.end_set is not None:
            episode_ended = state in self.end_set 
        else:
            episode_ended = len( self.Q[ state ] ) == 0

        if episode_ended:
            state = self._start()
        self.state = state

        return state, reward, episode_ended

    def to_graph( self ):
        """Create a graph from the MDP environment"""

        graph = nx.MultiDiGraph()
        # Add all states as nodes
        for i in xrange( self.S ):
            graph.add_node( i )
        for a in xrange( self.A ):
            # Add pr-edges for each action
            for i in xrange( self.S ):
                for (j,pr) in self.P[ a ][ i ]:
                    graph.add_edge( i, j, pr = pr, action = a )

        return graph

    def to_dot( self ):
        """Create a graph from the MDP environment"""

        s = ""
        s += "# Autogenerated rl-domain graph\n"
        s += "digraph{ \n"

        # Add a node for all states
        for i in xrange( self.S ):
            s += '%d [label=""];\n'%( i )
        # Add pr-edges
        for a in xrange( self.A ):
            # Add pr-edges for each action
            for i in xrange( self.S ):
                for (j,pr) in self.P[ a ][ i ]:
                    s += "%d -> %d;\n"%( i, j )
        s += "}\n"
        return s

class Option:
    r"""Encapsulates an option: I, \pi, \beta"""
    I = set([])
    pi = {}
    B_ = {}

    def __init__( self, I, pi, B ):
        self.I = I
        self.pi = pi
        self.B_ = B

    def __repr__(self):
        return "[Option: %s]"%( id( self ) )

    def can_start( self, state ):
        return state in self.I

    def act( self, state ):
        action = util.choose( self.pi[ state ] )
        return action

    def B( self, state ):
        if state in self.B_:
            return self.B_[ state ]
        elif state in self.pi and len( self.pi[ state ] ) > 0:
            return 0.0
        else:
            return 1.0

    def should_stop( self, state ):
        b = self.B( state )
        if b == 1.0:
            return True
        elif b == 0.0:
            return False
        elif np.random.random() < b:
            return True
        else:
            return False

class OptionEnvironment( Environment ):
    """
    Environment that also supports options defines a graph structure
    Note: We don't save actions as options from an efficiency standpoint.
    """
    O = []

    @staticmethod
    def make_point_option( g, gr, dest ):
        """Create an option that takes all connected states to dest"""
        # Get all paths
        paths = nx.shortest_path( gr, source=dest )

        I = set( paths.keys() )
        I.remove( dest )
        pi = {}
        for src, path in paths.items():
            if src == dest: continue
            # Next link in the path
            dest_ = path[ -2 ]

            # Choose the maximum probability action for this edge
            actions = [ (attrs['action'], attrs['pr']) for src, dest__, attrs in g.edges( src, data=True ) if dest__ == dest_ ] 
            action = max( actions, key = lambda (a,pr): pr )[ 0 ]

            pi[ src ] = ((action, 1.0),)
        B = {}
        
        return Option( I, pi, B )

    @staticmethod
    def make_path_option( g, gr, start, dest ):
        """Create an option that takes a state to a dest"""

        o = OptionEnvironment.make_point_option( g, gr, dest )
        # Start not reachable from dest
        if not start in o.I:
            return None
        else:
            o.I = set( [start] )

        return o

    @staticmethod
    def make_markov_path_option( g, gr, start, dest ):
        """Create an option that takes a state to a dest"""
        # Get path
        path = nx.shortest_path( g, source=start, target=dest )

        I = set( path )
        I.remove( dest )

        pi = {}
        for i in xrange( len( path ) - 1 ):
            # Next link in the path
            j = i+1
            src = path[ i ]
            dest_ = path[ j ]

            # Choose the maximum probability action for this edge
            actions = [ (attrs['action'], attrs['pr']) for src, dest__, attrs in g.edges( src, data=True ) if dest__ == dest_ ] 
            action = max( actions, key = lambda (a,pr): pr )[ 0 ]

            pi[ src ] = ((action, 1.0),)
        B = {}
        
        return Option( I, pi, B )

    @staticmethod
    def make_options_from_random_nodes( g, gr, count ):
        """Create an option that takes a state to a random set of nodes"""
        nodes = gr.nodes()
        random.shuffle( nodes )
        nodes = nodes[ :count ]

        # Get paths to node
        return [ OptionEnvironment.make_point_option( g, gr, n ) for n in nodes ]

    @staticmethod
    def make_options_from_betweenness( g, gr, count ):
        """Create an option that takes a state to a random set of nodes"""

        # Get betweenness scores
        bw = nx.betweenness_centrality( g )

        local_maximas = []
        for n, b in bw.items():
            for n_ in g.successors( n ):
                if bw[ n_ ] > b: break
            else:
                local_maximas.append( (n, b) )
        local_maximas.sort( key = lambda (x,v): -v )
        local_maximas = [ x for (x,v) in local_maximas[ :count ] ]

        # Get paths to node
        return [ OptionEnvironment.make_point_option( g, gr, n ) for n in local_maximas ]

    @staticmethod
    def make_options_from_random_paths( g, gr, count, markov ):
        """Create an option that takes a state to a random set of nodes"""

        # Get all the edges in the graph
        nodes = g.nodes()
        random.shuffle( nodes )

        options = []
        for node in nodes:
            if len( options ) > count: 
                break

            neighbours = g.successors( node )
            if len( neighbours ) == 0: 
                continue
            dest = random.choice( neighbours )
            # TODO: Prevent choosing subsumed paths
            if markov:
                o = OptionEnvironment.make_markov_path_option( g, gr, node, dest )
            else:
                o = OptionEnvironment.make_path_option( g, gr, node, dest )
            if o:
                options.append( o )

        # Get paths to node
        return options

    @staticmethod
    def make_options_from_small_world( g, gr, count, markov, r = None ):
        """Create an option that takes a state to a random nodes as per a power-law dist"""

        if r is None:
            # Estimate as avg degree / 2 == edges / nodes
            r = len( g.edges() ) / float( len( g.nodes() ) )

        # Get all the edges in the graph
        path_lengths = nx.shortest_path_length( g ).items()
        random.shuffle( path_lengths )

        options = []
        for node, dists in path_lengths:
            if len( options ) > count: 
                break

            dists.pop( node )
            if not dists: 
                continue
            neighbours, dists = zip( *dists.items() )
            # Create a pr distribution
            dists = np.power( np.array( dists, dtype=float ), r )
            # Zero out neighbours
            for i in xrange( len( dists ) ):
                if dists[i] == 1: dists[i] = 0
            if not dists.any(): 
                continue
            dest = util.choose( zip( neighbours, dists ) )
            # TODO: Prevent choosing subsumed paths
            if markov:
                o = OptionEnvironment.make_markov_path_option( g, gr, node, dest )
            else:
                o = OptionEnvironment.make_path_option( g, gr, node, dest )
            if o:
                options.append( o )

        # Get paths to node
        return options
   
    def __init__( self, S, A, P, R, R_bias, start_set, end_set, O ):
        Environment.__init__( self, S, A, P, R, R_bias, start_set, end_set )
        self.O = O

        # Update the Q function based on the options we now have
        Q = []
        for s in xrange(self.S):
            actions = tuple( ( a for a in xrange( self.A ) if len( self.P[ a ][ s ] ) > 0 ) )
            options = tuple( ( o for o in O if s in o.I ) )
            Q.append( actions + options )
        self.Q = Q

    def react( self, action ):
        """
        React to action
        @returns new state and valid actions, and reward, and if episode has
        ended
        """

        if isinstance( action, Option ):
            option = action
            history = []
            rewards = []

            # Act according to the option
            action = option.act( self.state )
            history.append( ( self.state, action ) )

            state, reward, episode_ended = self._react( action )
            rewards.append( reward )

            while not episode_ended and not option.should_stop( state ):
                # Use the option policy 
                action = option.act( state )
                history.append( ( state, action ) )

                state, reward, episode_ended = self._react( action )
                rewards.append( reward )

            history.append( (state, None) )

            return history, tuple(rewards), episode_ended
                
        else:
            return self._react( action )

